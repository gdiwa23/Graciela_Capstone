{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c4dd3ba-b488-4cbc-b169-b0475cf23dc2",
   "metadata": {},
   "source": [
    "## Cleaning and NLP \n",
    "* sensational corpus is the transcript for the episodes that we have\n",
    "* scientific corpus is the wiki descriptions + fishbase\n",
    "* document is the full contents of wiki page/fishbase page, or transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f37b654-0ac2-46e4-8605-0b3396a9839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# regex !!\n",
    "import re\n",
    "\n",
    "#display\n",
    "from IPython.display import display, Markdown\n",
    "from PIL import Image \n",
    "\n",
    "# words / NLP\n",
    "from gensim.corpora import Dictionary   # import dictionary of english words from gensim, will help w tfidf\n",
    "from wordcloud import WordCloud \n",
    "import unicodedata                      # for dealing w non ascii !\n",
    "\n",
    "import nltk                             # famous nl libraries \n",
    "from nltk.corpus import stopwords       # curated list of english stopwords\n",
    "from nltk.tokenize import word_tokenize # tokenisation function \n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer # lemmatiser\n",
    "\n",
    "# !pip install sentence-transformers\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "#from IPython.display import Audio\n",
    "#positive_chime = Audio(filename='positive_chime.wav', autoplay=True) ## lil alarm for me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ba8a56-54b7-4e5a-8298-964c056be226",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 1: Quick sanity checks, get to know the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "133a2fb6-dd78-44a0-87f3-3b9a8735934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('RM_complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9958fe9c-8edb-4989-a8a5-a4c872b0021b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**--- .info Overview ---**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19 entries, 0 to 18\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   episode_name  19 non-null     object\n",
      " 1   english_name  19 non-null     object\n",
      " 2   latin_name    19 non-null     object\n",
      " 3   transcript    19 non-null     object\n",
      " 4   wiki_desc     19 non-null     object\n",
      "dtypes: object(5)\n",
      "memory usage: 892.0+ bytes\n",
      "None\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**--- % Missing Values ---**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_name    0.0\n",
      "english_name    0.0\n",
      "latin_name      0.0\n",
      "transcript      0.0\n",
      "wiki_desc       0.0\n",
      "dtype: float64\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**No duplicate rows: True**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Empty Transcripts: 0**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Empty wiki_desc: 0**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**--- Transcript Length Stats ---**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       cc_transcript  wc_transcript\n",
      "count      19.000000      19.000000\n",
      "mean    29975.526316    5496.473684\n",
      "std      3535.963367     679.511701\n",
      "min     24742.000000    4564.000000\n",
      "25%     27520.000000    5056.500000\n",
      "50%     28962.000000    5279.000000\n",
      "75%     32615.000000    5955.500000\n",
      "max     36801.000000    6889.000000\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**--- wiki_desc Stats ---**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            cc_wiki      wc_wiki\n",
      "count     19.000000    19.000000\n",
      "mean   14701.631579  2360.000000\n",
      "std     6566.906300  1078.232144\n",
      "min     5235.000000   839.000000\n",
      "25%     8654.000000  1366.500000\n",
      "50%    14552.000000  2298.000000\n",
      "75%    19146.500000  3044.500000\n",
      "max    27322.000000  4409.000000\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**--- Episode Counts ---**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_name\n",
      "Demon Fish              1\n",
      "Amazon Assassins        1\n",
      "Silent Assassin         1\n",
      "Jungle Killer           1\n",
      "Hidden Predator         1\n",
      "Flesh Ripper            1\n",
      "Electric Executioner    1\n",
      "Chainsaw Predator       1\n",
      "Amazon Flesh Eaters     1\n",
      "European Maneater       1\n",
      "Death Ray               1\n",
      "Alligator Gar           1\n",
      "Killer Catfish          1\n",
      "Piranha                 1\n",
      "Rift Valley Killer      1\n",
      "Alaskan Horror          1\n",
      "Congo Killer            1\n",
      "Killer Snakehead        1\n",
      "The Mutilator           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Transcripts with non-ASCII characters: 11**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**wiki_desc with non-ASCII characters: 19**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Transcripts with subtitle timecodes: 0**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Longest episode: Alaskan Horror**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Shortest episode: Chainsaw Predator**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# quick function to wrap all these wuick checks into one\n",
    "\n",
    "def sanity_check_transcripts(df):\n",
    "    \"\"\"\n",
    "    Run quick sanity checks on df.\n",
    "    Adds 'char_count' and 'word_count' columns for analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    display(Markdown(\"**--- .info Overview ---**\"))\n",
    "    print(df.info())\n",
    "    print('\\n')\n",
    "    \n",
    "    # % of missing values in each column\n",
    "    display(Markdown(\"**--- % Missing Values ---**\"))\n",
    "    print(df.isnull().sum() / len(df) * 100)\n",
    "    print('\\n')\n",
    "\n",
    "    # Check for exact duplicates\n",
    "    no_duplicates = df.shape[0] == df.drop_duplicates().shape[0]\n",
    "    display(Markdown(f\"**No duplicate rows: {no_duplicates}**\\n\"))\n",
    "\n",
    "    # Check for empty transcript strings\n",
    "    empty_count = (df['transcript'].str.strip() == '').sum()\n",
    "    display(Markdown(f\"**Empty Transcripts: {empty_count}**\\n\"))\n",
    "\n",
    "    empty_count = (df['wiki_desc'].str.strip() == '').sum()\n",
    "    display(Markdown(f\"**Empty wiki_desc: {empty_count}**\\n\"))\n",
    "    print('\\n')\n",
    "\n",
    "    # Transcript length stats\n",
    "    df['cc_transcript'] = df['transcript'].str.len()\n",
    "    df['wc_transcript'] = df['transcript'].str.split().str.len()\n",
    "    display(Markdown(\"**--- Transcript Length Stats ---**\"))\n",
    "    print(df[['cc_transcript', 'wc_transcript']].describe())\n",
    "    print('\\n')\n",
    "    \n",
    "    # Wiki_desc stats\n",
    "    df['cc_wiki'] = df['wiki_desc'].str.len()\n",
    "    df['wc_wiki'] = df['wiki_desc'].str.split().str.len()\n",
    "    display(Markdown(\"**--- wiki_desc Stats ---**\"))\n",
    "    print(df[['cc_wiki', 'wc_wiki']].describe())\n",
    "    print('\\n')\n",
    "\n",
    "    # Episodes count check\n",
    "    display(Markdown(\"**--- Episode Counts ---**\"))\n",
    "    print(df['episode_name'].value_counts())\n",
    "    print('\\n')\n",
    "\n",
    "    # Non-ASCII character check\n",
    "    weird_chars = df['transcript'].apply(lambda x: re.findall(r'[^\\x00-\\x7F]+', x))\n",
    "    weird_lines = weird_chars[weird_chars.str.len() > 0]\n",
    "    display(Markdown(f\"**Transcripts with non-ASCII characters: {len(weird_lines)}**\\n\"))\n",
    "\n",
    "    # Non-ASCII character check\n",
    "    weird_chars = df['wiki_desc'].apply(lambda x: re.findall(r'[^\\x00-\\x7F]+', x))\n",
    "    weird_lines = weird_chars[weird_chars.str.len() > 0]\n",
    "    display(Markdown(f\"**wiki_desc with non-ASCII characters: {len(weird_lines)}**\\n\"))\n",
    "    \n",
    "\n",
    "    # Check for leftover timecodes like 00:01:23,456\n",
    "    timecode_mask = df['transcript'].str.contains(r'\\d{2}:\\d{2}:\\d{2},\\d{3}')\n",
    "    timecode_count = timecode_mask.sum()\n",
    "    display(Markdown(f\"**Transcripts with subtitle timecodes: {timecode_count}**\\n\"))\n",
    "\n",
    "\n",
    "    # Longest & shortest episode names by word count\n",
    "    longest_ep = df.loc[df['wc_transcript'].idxmax(), 'episode_name']\n",
    "    shortest_ep = df.loc[df['wc_transcript'].idxmin(), 'episode_name']\n",
    "    display(Markdown(f\"**Longest episode: {longest_ep}**\\n\"))\n",
    "    display(Markdown(f\"**Shortest episode: {shortest_ep}**\\n\"))\n",
    "    print('\\n')\n",
    "\n",
    "    ## drop 'Unnamed: 0.1', 'Unnamed: 0'\n",
    "    \n",
    "    #df = df.drop(columns = ['Unnamed: 0.2', 'Unnamed: 0.1','Unnamed: 0'])\n",
    "\n",
    "    return df  # Returns df with char_count & word_count columns added\n",
    "\n",
    "# check the data and also add word counts\n",
    "df = sanity_check_transcripts(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613dfe26-c564-42d3-9009-db00f7805760",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 2: Light Cleaning - preparing for embedded semantic\n",
    "* check for non ascii and sort out\n",
    "* cast text cols (objects) to strings\n",
    "* prepare for embedded semantic filtering\n",
    "  * stopwords not removed **yet** bc can be important for context at this stage\n",
    "  \n",
    " **regex remove**:\n",
    "* '\\n'\n",
    "* '...'\n",
    "* lowercase all\n",
    "* remove trailing spaces\n",
    "* Strip timecodes, HTML tags, captions\n",
    "    * import unicodedata - for dealing w ascii like a smart person # df['transcript'] = df['transcript'].apply(lambda x: unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode())\n",
    "* Optional: Lemmatize .. maybe do that later !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53cdd42b-e350-405f-9ac4-faa0cea453cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for non ascii characters\n",
    "def ascii_check(df):\n",
    "    \n",
    "    # Non-ASCII character check\n",
    "    weird_chars = df['transcript'].apply(lambda x: regex.findall(r'[^\\x00-\\x7F]+', x))\n",
    "    weird_lines = weird_chars[weird_chars.str.len() > 0]\n",
    "    display(Markdown(f\"**Transcripts with non-ASCII characters: {len(weird_lines)}**\\n\"))\n",
    "        \n",
    "    # Non-ASCII character check\n",
    "    weird_chars = df['wiki_desc'].apply(lambda x: regex.findall(r'[^\\x00-\\x7F]+', x))\n",
    "    weird_lines = weird_chars[weird_chars.str.len() > 0]\n",
    "    display(Markdown(f\"**wiki_desc with non-ASCII characters: {len(weird_lines)}**\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a719a56-e3a5-493c-a33d-9d2b8bdaa075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "\n",
    "    def remove_non_ascii(text):\n",
    "        # Normalize the text to NFKD form\n",
    "        normalized = unicodedata.normalize('NFKD', text)\n",
    "        # Encode to ASCII bytes, ignoring characters that can't be converted\n",
    "        ascii_bytes = normalized.encode('ascii', 'ignore')\n",
    "        # Decode back to string\n",
    "        return ascii_bytes.decode('ascii')\n",
    "\n",
    "    for col in ['transcript', 'wiki_desc']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(remove_non_ascii)\n",
    "            print(f\"Processed column: {col}\")  # debug\n",
    "        \n",
    "    # optional: run ascii_check if you want to verify\n",
    "    ascii_check(df)\n",
    "    return df\n",
    "\n",
    "#df = clean_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f58abddd-7b7f-445e-8cea-ca3cb24d4569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19 entries, 0 to 18\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   episode_name   19 non-null     object\n",
      " 1   english_name   19 non-null     object\n",
      " 2   latin_name     19 non-null     object\n",
      " 3   transcript     19 non-null     object\n",
      " 4   wiki_desc      19 non-null     object\n",
      " 5   cc_transcript  19 non-null     int64 \n",
      " 6   wc_transcript  19 non-null     int64 \n",
      " 7   cc_wiki        19 non-null     int64 \n",
      " 8   wc_wiki        19 non-null     int64 \n",
      "dtypes: int64(4), object(5)\n",
      "memory usage: 1.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info() ## all objects to to string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f65fae6b-dbe2-4a29-92fe-a21b9b0d5b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casted 'episode_name' to string dtype.\n",
      "Casted 'english_name' to string dtype.\n",
      "Casted 'latin_name' to string dtype.\n",
      "Casted 'transcript' to string dtype.\n",
      "Casted 'wiki_desc' to string dtype.\n"
     ]
    }
   ],
   "source": [
    "# cast to string\n",
    "def enforce_string_dtype(df, text_cols=('episode_name', 'english_name','latin_name', 'transcript', 'wiki_desc')):\n",
    "    \"\"\"\n",
    "    Permanently cast specified columns to Pandas string dtype.\n",
    "    \"\"\"\n",
    "    for col in text_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(\"string\")\n",
    "            print(f\"Casted '{col}' to string dtype.\")\n",
    "    return df\n",
    "\n",
    "# usage\n",
    "df = enforce_string_dtype(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf5a4741-8009-41b6-adda-d4a8106a1f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuation for regex\n",
    "string.punctuation # punctuation loaded in alr # nice \n",
    "punc = string.punctuation # use punc \n",
    "punc = punc.replace(\"-\", \"\") ## this will keep double barrelled words for semantic analysis , may have to remove later !\n",
    "punc; # - removed, should preserve double barrrelled words for now\n",
    "\n",
    "\n",
    "# function for cleaning\n",
    "def regex_clean(txt, pattern_str):\n",
    "    reg = re.compile(pattern_str)\n",
    "    return \" \".join(reg.sub(\" \", txt).split())\n",
    "\n",
    "# function to prep text data for embedding\n",
    "def embedprep_data(text):     \n",
    "    \"\"\" Cleans up text!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : string\n",
    "        A text string that you want to parse and remove regex pattern matches \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Cleaned up string, ready for semantic filtering\n",
    "    \"\"\"   \n",
    "\n",
    "    text = text.lower()                                 # all text to lowercase\n",
    "    text = regex_clean(text, r'\\s\\d+\\s')                # remove numbers and handle spacing issues\n",
    "    text = regex_clean(text, r'\\s\\d*\\.\\d+\\s')           # capture floats\n",
    "    text = regex_clean(text, r'\\s?(\\d*\\.\\d+)\\s?')       # currency\n",
    "    text = regex_clean(text, r'\\s?(\\d+(?:\\.\\d+)?)\\s?%') # handle percentages\n",
    "    text = regex_clean(text, r'\\.\\.\\.')                 # handles the ellipses\n",
    "\n",
    "\n",
    "    text = \"\".join([char for char in text if char not in punc]) # remove all punctuation, except '-'\n",
    "    \n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def prep_embed(df):\n",
    "    cols = df[['episode_name', 'english_name','latin_name','transcript', 'wiki_desc']]\n",
    "    for col in cols:\n",
    "        df[col] = df[col].apply(embedprep_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0fa9a98-0cc2-4881-8618-32a189e8dc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightly cleans, prepares for embedded semantic filtering\n",
    "\n",
    "df_embed = prep_embed(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd8cc6af-76b7-41fc-a004-238821862a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_embed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b90dc92-9f1e-4325-8513-eec090280423",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 3: Embedded semantic filtering -> keep only relevant chunks\n",
    "\n",
    "### embedding based semantic filtering\n",
    "* instantiate embedder from sentence-transformer\n",
    "1) have a lightly cleaned df, hyphens preserved\n",
    "2) handle variants for english_name and latin_name\n",
    "3) use variants of names to create a semantic pool, also w prompts for 'concept' of species ** go back and add to concept of fish!\n",
    "4) chunk the text to maintain context when 'reading'\n",
    "5) cosine similary of chunk to embeddings queries/prompts, also uses keyword_gate so important chunks aren't missed just bc they dont meet the threshold -> keeps only important chunks !\n",
    "6) stitch the chunks together to be more coheerent\n",
    "7) 8) get em working !!!!! done it to test df, apply to the rest !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1c31bf5-37ac-46d4-a17b-32c7340f728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df_embed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46b5a478-8dc8-4beb-ae67-31ca67b52a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed filtering\n",
    "# using sentence-transformer model !\n",
    "\n",
    "\n",
    "# instatiate\n",
    "class Embedder:\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    def __call__(self, texts):\n",
    "        # Return L2-normalized numpy array (n, d)\n",
    "        arr = self.model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "        return arr\n",
    "\n",
    "embed = Embedder()  # instantiate once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "271cb94f-1884-4ae4-9e86-10eb6affab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: df_embed = ready and lightly cleaned ! \n",
    "\n",
    "\n",
    "################################################\n",
    "\n",
    "\n",
    "\n",
    "# step 2: english_name variants ------ make sure to do to latin name for wiki_desc\n",
    "def expand_name_variants(english_name: str, latin_name: str) -> set: #take in eng and lat names\n",
    "    variants = set()\n",
    "\n",
    "    # english name variants (multi-word, no hyphens)\n",
    "    if english_name and isinstance(english_name, str):      # if str, carry on\n",
    "        eng = english_name.strip().lower()                  # strip and lower case # red bellied piranha\n",
    "        hyphen_first = eng.replace(\" \", \"-\", 1)                 # red-bellied piranha\n",
    "        split_name = eng.split()                                # single words\n",
    "        variants |= {eng, eng.replace(\" \", \"-\"), hyphen_first, \" \".join(split_name)}  # red-bellied-piranha\n",
    "\n",
    "    # Latin name variants\n",
    "    if latin_name and isinstance(latin_name, str):\n",
    "        lat = latin_name.strip()\n",
    "        variants |= {lat.lower(), lat.replace(\" \", \"-\").lower()} # clarias gariepinus, clarias-gariepinus\n",
    "\n",
    "        # Abbreviated genus: Clarias gariepinus -> C. gariepinus\n",
    "        parts = lat.split()\n",
    "        if len(parts) == 2:\n",
    "            genus, species = parts\n",
    "            abbrev = f\"{genus[0]}. {species}\".lower()  #c. gariepinus\n",
    "            abbrev2= f\"{genus[0]}.{species}\".lower()   #c.gariepinus\n",
    "            variants |= {abbrev, abbrev2} # add to variants \n",
    "\n",
    "    return variants\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################\n",
    "\n",
    "\n",
    "# step 3 : \n",
    "def build_queries(english_name: str, latin_name: str = \"\") -> list[str]:\n",
    "    #aliases = aliases or [] # no aliases\n",
    "    pool = set()\n",
    "    pool |= expand_name_variants(english_name or \"\", latin_name or \"\")\n",
    "\n",
    "    # add short “concept” prompts to anchor meaning -\n",
    "    prompts = [\n",
    "\n",
    "        # core \n",
    "        f\"{english_name} {latin_name}\".strip(),\n",
    "        f\"{english_name} species\".strip() if english_name else \"\",\n",
    "        f\"{latin_name} fish\".strip() if latin_name else \"\",\n",
    "        'fish',\n",
    "\n",
    "        # include \n",
    "        'jaws', 'razor-sharp', 'muscle', \"markings\", \"predator\",\\\n",
    "        'use its'\n",
    "\n",
    "    ]\n",
    "    pool |= {embedprep_data(p) for p in prompts if p} # cleans the text, eg lowercasing in case and adde to pool\n",
    "    \n",
    "    # keep very short queries for sharper signal\n",
    "    return sorted({q for q in pool if q and len(q.split()) <= 6})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################\n",
    "\n",
    "\n",
    "\n",
    "# step 4: \n",
    "def chunk_text(text: str, words_per_chunk=100, overlap=20): # chunks , w overlap to maintain context\n",
    "    \"\"\"\n",
    "    Split into overlapping word chunks: [(chunk_text, start_idx, end_idx), ...]\n",
    "    \"\"\"\n",
    "    t = embedprep_data(text or \"\") # normalise in case, emppty instead of crashing    \n",
    "    words = t.split() # splittext into indiv words\n",
    "    if not words:\n",
    "        return [] # in case empty\n",
    "        \n",
    "    chunks = [] # list for chunks\n",
    "    step = max(1, words_per_chunk - overlap) # move through chunks, max(1 to stop infinite loop\n",
    "    for start in range(0, len(words), step): # loop through using step for size\n",
    "        end = min(len(words), start + words_per_chunk) # end is at end\n",
    "        chunks.append((\" \".join(words[start:end]), start, end)) # join chunked words back to str\n",
    "        if end == len(words): # end loop\n",
    "            break\n",
    "    return chunks # return the chnks \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# step5 : score chunks vs queries (cosine on normalised vectors) \n",
    "\n",
    "keyword_gate = ['fish', 'teeth', 'fins', 'scales', 'body'] # keep chunk that has any \n",
    "\n",
    "\n",
    "# chunk_emb = vector embedding, query embs = vectors for queries/prompts \n",
    "# function calc cosine similarity between chunks and query\n",
    "def cosine_max_score(chunk_emb: np.ndarray, query_embs: np.ndarray) -> float:\n",
    "    # embeddings are L2-normalized, so dot product = cosine similarity\n",
    "    return float(np.max(query_embs @ chunk_emb)) # find and returns max, how well chunk matches queries\n",
    "\n",
    "\n",
    "\n",
    "# func for selecting the relevant chunks !!. threshold can be changed !, \n",
    "# should be changed, esp if we have such a long list of prompts\n",
    "def select_relevant_chunks(chunks,        #text chnks\n",
    "                           query_embs,    # embeddings of q/prompts\n",
    "                           embed_fn,      # text to embeddings\n",
    "                           threshold=0.5, # mess about w this one\n",
    "                           top_k=None,    # keep stop scoring chunks, may not need bc very lil data\n",
    "                           keyword_gate: set | None = None): # if appear, keep regardless fo score\n",
    "    \"\"\"\n",
    "    - Embed all chunks once\n",
    "    - Keep chunk if score >= threshold\n",
    "    - Optionally keep top_k chunks regardless of threshold\n",
    "    - Optional cheap keyword gate: if any keyword appears in the chunk text, keep it\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return [] # handle in case empty\n",
    "    texts = [c[0] for c in chunks]\n",
    "    chunk_embs = embed_fn(texts)  # embed all chunks using func\n",
    "    \n",
    "    selected = []\n",
    "    \n",
    "    for i, (text, s, e) in enumerate(chunks): #loop thru chunks, calc max cosine sim\n",
    "        score = cosine_max_score(chunk_embs[i], query_embs) # using cosine_max_score\n",
    "        keep = score >= threshold     # keep chunk ?\n",
    "        if not keep and keyword_gate:\n",
    "            # lowercase containment check (text already normalized)\n",
    "            if any(k in text for k in keyword_gate): # check keyword_gate = keep regardless of threshold value\n",
    "                keep = True \n",
    "        if keep:\n",
    "            selected.append((text, s, e, score)) ### chunk, start, end, similarity score\n",
    "\n",
    "    return selected\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################\n",
    "\n",
    "\n",
    "\n",
    "# step 6: merge adjacent selected chunks into coherent sections\n",
    "def merge_adjacent_chunks(selected, gap_words=20):  #### can change gapwords\n",
    "    if not selected:\n",
    "        return \"\" # if empty\n",
    "    merged = []\n",
    "    cur_text, cur_s, cur_e, _ = selected[0] # start group, current chunk s and e\n",
    "    for text, s, e, _score in selected[1:]: # check chunk, see if join to group\n",
    "        if s - cur_e <= gap_words: # treet as adjacent , concat \n",
    "            cur_text = cur_text + \" \" + text\n",
    "            cur_e = e \n",
    "        else:\n",
    "            merged.append(cur_text) # extend e\n",
    "            cur_text, cur_s, cur_e = text, s, e # close group start new chunk\n",
    "    merged.append(cur_text) # save final text \n",
    "    return \"\\n\\n\".join(merged) # return saved text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## step 7) filter the text ,, add dynamic settings to handle range of text length, third filter text, trial\n",
    "def filter_3(\n",
    "    text, english_name, latin_name, embed\n",
    "):\n",
    "    \"\"\"\n",
    "    Filter text using embeddings with dynamic chunk size, overlap, and threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    # dynamic settings based on word_count\n",
    "    word_count = len(text.split())\n",
    "    if word_count < 300: # for shorter text eg some of the wikis are just 200 ish\n",
    "        chunk_size = 40\n",
    "        overlap = 10\n",
    "        threshold = 0.48\n",
    "    elif word_count < 2000: ## mid siz\n",
    "        chunk_size = 80\n",
    "        overlap = 15\n",
    "        threshold = 0.52\n",
    "    else: \n",
    "        chunk_size = 150 # for longer text\n",
    "        overlap = 30\n",
    "        threshold = 0.57\n",
    "\n",
    "    # queries and embeddings\n",
    "    queries = build_queries(english_name, latin_name)\n",
    "    if not queries:\n",
    "        return \"\"\n",
    "    query_embs = embed(queries)\n",
    "    gate = set(queries) # keep if they appear regardless fo thresh\n",
    "\n",
    "    # chunk text using func, break text up w some overlap for context\n",
    "    chunks = chunk_text(text, words_per_chunk=chunk_size, overlap=overlap)\n",
    "    if not chunks:\n",
    "        return \"\"\n",
    "\n",
    "    # relevant chunks, calcs cosine similarity or uses keywords\n",
    "    selected = select_relevant_chunks( \n",
    "        chunks,              # \n",
    "        query_embs,\n",
    "        embed_fn=embed,\n",
    "        threshold=threshold, # will be adjusted\n",
    "        keyword_gate=gate\n",
    "    )\n",
    "    if not selected:\n",
    "        return \"\" ## only 'relevant' chunks remain\n",
    "\n",
    "    # merge without duplicating , combine chunks backt o one text\n",
    "    merged_texts = []\n",
    "    last_end = -1\n",
    "    for text_chunk, start, end, _ in selected:\n",
    "        if start <= last_end:\n",
    "            text_chunk = \" \".join(text_chunk.split()[last_end - start + 1:])\n",
    "        if text_chunk:\n",
    "            merged_texts.append(text_chunk)\n",
    "        last_end = end\n",
    "\n",
    "    merged = \"*****************\".join(merged_texts)\n",
    "\n",
    "    # remove any repeated phrases in case\n",
    "    alr = set()\n",
    "    final_text = []\n",
    "    for sentence in merged.split(\". \"): # splits into sentences\n",
    "        if sentence not in alr:\n",
    "            alr.add(sentence) # nop duplicate texts\n",
    "            final_text.append(sentence)\n",
    "    return \". \".join(final_text) # clean text final\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# add new counts\n",
    "def add_counts(df, transcript_col=\"transcript_embed\", wiki_col=\"wiki_desc_embed\"):\n",
    "\n",
    "    # Word counts\n",
    "    df[\"cc_transcript_embed\"] = df[transcript_col].apply(len)\n",
    "    df[\"wc_transcript_embed\"] = df[transcript_col].apply(lambda x: len(x.split()))\n",
    "\n",
    "    df[\"cc_wiki_desc_embed\"] = df[wiki_col].apply(len)\n",
    "    df[\"wc_wiki_desc_embed\"] = df[wiki_col].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f64d5777-af36-44a8-969b-007d890d441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## wrapper func !! \n",
    "\n",
    "def filter_df_dynamic(\n",
    "    df: pd.DataFrame,\n",
    "    english_col=\"english_name\",\n",
    "    latin_col=\"latin_name\",\n",
    "    transcript_col=\"transcript\",\n",
    "    wiki_col=\"wiki_desc\"\n",
    ") -> pd.DataFrame:\n",
    "    embed = Embedder()  # load model once\n",
    "    df = df.copy()\n",
    "    new_transcripts, new_wikis = [], []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        english, latin = row[english_col], row[latin_col]\n",
    "\n",
    "        # filter transcript\n",
    "        transcript = row.get(transcript_col, \"\")\n",
    "        new_transcripts.append(filter_3(transcript, english, latin, embed=embed))\n",
    "\n",
    "        # filter wiki description\n",
    "        wiki = row.get(wiki_col, \"\")\n",
    "        new_wikis.append(filter_3(wiki, english, latin, embed=embed))\n",
    "\n",
    "    df[\"transcript_embed\"] = new_transcripts\n",
    "    df[\"wiki_desc_embed\"] = new_wikis\n",
    "    return df\n",
    "\n",
    "#test_new = filter_df_dynamic(test_df)\n",
    "#test_new = add_counts(test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d33f4b8-d8b5-42c8-9555-d1843353fa72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 4: more cleaning !\n",
    "* now we have the relevant chunks, let's extract the descriptors !\n",
    "\n",
    "### POS and word extraction\n",
    "* switched to this approach after standalone adjectives were not powerful enough \n",
    "\n",
    "\n",
    "**using spacy POS tagging but this time expanding extraction to include**\n",
    "* Noun, nouns+adj, chunks arounud nouns,\n",
    "* specified anatomic words that we wanted to keep eg fins, teeth\n",
    "* excluded name variants ! no leakage !\n",
    "* reduced noise further by having an extensive list of unwanted words\n",
    "* boosted the weightings of words specific to fish, and multiple word phrases that included them = more informative\n",
    "\n",
    "\n",
    "**top desscriptors have been extracted! ready for imagen/vertex**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31cf9f73-e9a0-4aa1-bfab-88e85c36ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using lemmatiser bc stemmers produce nonsense words not useful for prompts\n",
    "\n",
    "lem = WordNetLemmatizer() # lemmatiser\n",
    "stpwrd = nltk.corpus.stopwords.words('english')  ## from nltk, common stopwords\n",
    "stpwrd.extend(string.punctuation)    # from nltk , extend spwrds to include punctuation, remove simultaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307ec7d4-a148-4c27-8a6f-18d6a5ecda8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nlp , df\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "df = test_new.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7a7068a-de84-416f-9dc4-18119b3fd4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "\n",
    "# --- helpers ---\n",
    "## built from words that stood out as uninformative\n",
    "STOP_DESCRIPTORS = {\n",
    "    \"species\", \"animal\", \"organism\", \"creature\",\n",
    "    \"head\", \"water\", \"sea\", \"ocean\", \"river\", \"deep\",\n",
    "    \"african\", \"european\", \"asian\", \"southern\", \"northern\",\n",
    "    \"jungle\", 'little','strain','a','treble','crocodile','girl',\n",
    "    'father','fish','ft','ufo','agh','my','wife','biologist','child',\n",
    "    'fishery','frogs','dog','amazon', 'environmental','earth','aiiigator',\n",
    "    'fisherman','scientists','pets','','carp','daddy','dad','snail','fishing',\n",
    "    'your','village','his','jeremy','wade','lake','alaskan', 'hippopotamus','river',\n",
    "    'a', 'this', 'the', 'that', 'some unfinished business', 'an','abandoned','could','its','somebody',\n",
    "    'final','leg','weight','certainly','information','their','fishing','nobody','certain','sockeye','industrial',\n",
    "    'detailed','continental','next','oldest','such','hoo','fingers','upper','lower','catfish','snakehead',\n",
    "    'looking','like','human','anal','culling', 'anal spines', 'anybody','adjoining','bull','and','then','another',\n",
    "    'meters','yards','word', 'reabsorption','overharvesting','lica','vegrandis','b','issue','data','reproduction',\n",
    "    'assumption','directions','air', 'k','reference','setting','any','signs','academy','award',\n",
    "    'actual','stocks','schooling','fisheries','days','conservation','air','aquaculture','aquarium',\n",
    "    'cocktail','fishs','these','teeth word','these','those','transformation','transform','details',\n",
    "    'quarters','retail','fortunes','lakes','economy','cargo','belgian','artificial',\n",
    "    'central illinois','fillets','cited examples','commercial importance', \n",
    "    'adult nile','adult nile perch','adult perch', 'anatomical', 'assistant', 'curator', 'bus',\n",
    "    'hoo', 'tech', 'bear', 'alligator', 'journalist', 'engine','physiology','few','aerated','birds mammals','flatworm dermopristis',\n",
    "    'brazil morphometrics','action malcolm douglas','bare hands', \n",
    "    'basket francisco','boys','brazils rugged frontier','brazilian rainforest','catching manner',\n",
    "    'anything fernando','bamboo rod'\n",
    "}\n",
    "STOP_DESCRIPTORS = {w.strip().lower() for w in STOP_DESCRIPTORS}\n",
    "\n",
    "\n",
    "## words that need to be included\n",
    "FISH_FEATURE_KEYWORDS = {\n",
    "    \"teeth\", \"tooth\", \"mouth\", \"jaw\", \"jaws\",\n",
    "    \"colour\", \"color\", \"hue\", \"colouration\"\n",
    "    \"pattern\", \"marking\",\"markings\", \"stripe\",\"striped\", \"spot\", \"band\",\n",
    "    \"body\", \"shape\", \"form\",\"shaped\",\"-like\", 'caudal',\n",
    "    \"fin\", \"spine\", \"spines\", \"barbels\", \"barbel\", \"whisker\",\"whiskers\", \"scale\",\"scales\", \n",
    "    \"plate\", \"head\", 'flat', \"snout\", \"tail\", 'torpedo-shaped', 'missile', 'elongated', 'flank',\n",
    "    'spiny', 'greyish', 'silver' \n",
    "}\n",
    "\n",
    "\n",
    "PERSONALITY_TERMS = {\n",
    "    \"elusive\", \"mysterious\", \"fearsome\", \"aggressive\", \"shy\",\n",
    "    \"timid\", \"curious\", \"bold\", \"hostile\", \"gentle\", \"peaceful\"\n",
    "}\n",
    "\n",
    "\n",
    "# words that need to be included\n",
    "FISH_LEXICON = {\n",
    "    \"anatomy\": [\"spines\", \"barbels\",\"whiskers\", 'flat', 'round',\n",
    "    \"plate\", \"armor\", \"snout\", \"tail\", 'torpedo-shaped', 'missile', 'elongated',\n",
    "        'flat', 'villiform teeth'\n",
    "        ],\n",
    "    \"ecology\": [\n",
    "        \"ambush\", \"bottom-dweller\", \"nocturnal\", \"schooling\",\n",
    "        \"migratory\", \"territorial\", \"camouflage\"\n",
    "    ]\n",
    "}\n",
    "LEXICON_TERMS = {term.lower() for group in FISH_LEXICON.values() for term in group}\n",
    "\n",
    "GENERIC_SINGLE_WORDS = {\"fish\", \"adult\", \"large\", \"big\", \"small\",'predator','predators','cut'}\n",
    "\n",
    "\n",
    "# examples of physical terms\n",
    "PHYSICAL_KEYWORDS = {\n",
    "    \"colour\": {\"grey\", \"silver\", \"olive\", \"golden\", \"blue\", \"dark\", \"light\", \"brown\", \"yellow\", \"green\", \"black\", \"white\", \"reddish\"},\n",
    "    \"pattern\": {\"striped\", \"spotted\", \"mottled\", \"banded\", \"dappled\", \"speckled\", \"marbled\"},\n",
    "    \"mouth_teeth\": {'sharp', 'needle-like', 'villiform','fangs',},\n",
    "    \"scales\": {\"scales\", \"scaly\", \"scaleless\", \"armour\", \"armor\"},\n",
    "    \"features\": {\"spines\", \"barbels\", \"whiskers\", \"snout\", \"plate\", \"tail\", \"fin\", \"fins\", \"dorsal\", \"caudal\", \"pectoral\"},\n",
    "    \"size\": {\"large\", \"giant\", \"small\", \"elongated\", \"torpedo\", \"flat\", \"round\", \"slender\", \"massive\", \"huge\",'long'},\n",
    "}\n",
    "PHYSICAL_TERMS = set().union(*PHYSICAL_KEYWORDS.values())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def expand_name_variants(english_name: str, latin_name: str) -> set:\n",
    "    variants = set()\n",
    "\n",
    "    # English name variants\n",
    "    if english_name and isinstance(english_name, str):\n",
    "        eng = english_name.strip().lower()\n",
    "        words = eng.split()\n",
    "        variants |= {\n",
    "            eng,\n",
    "            eng.replace(\" \", \"-\"),   # red-bellied-piranha\n",
    "            eng.replace(\" \", \"\"),    # redbelliedpiranha\n",
    "            *words                   # red, bellied, piranha\n",
    "        }\n",
    "\n",
    "    # Latin name variants\n",
    "    if latin_name and isinstance(latin_name, str):\n",
    "        lat = latin_name.strip().lower()\n",
    "        words = lat.split()\n",
    "\n",
    "        variants |= {\n",
    "            lat,\n",
    "            lat.replace(\" \", \"-\"),   # clarias-gariepinus\n",
    "            lat.replace(\" \", \"\"),    # clariasgariepinus\n",
    "            *words                   # clarias, gariepinus\n",
    "        }\n",
    "\n",
    "        if len(words) == 2:\n",
    "            genus, species = words\n",
    "            abbrev = f\"{genus[0]}. {species}\"\n",
    "            abbrev2 = f\"{genus[0]}.{species}\"\n",
    "            variants |= {abbrev, abbrev2}\n",
    "\n",
    "    # remove any accidental empties\n",
    "    return {v.strip().lower() for v in variants if v.strip()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def maybe_add(phrase, descriptors, blocked, force=False):\n",
    "    \"\"\"Clean + block check before adding a descriptor.\"\"\"\n",
    "    cleaned = clean_descriptor(phrase)\n",
    "    if not cleaned:\n",
    "        return\n",
    "\n",
    "    # normalize\n",
    "    cleaned_tokens = cleaned.split()\n",
    "\n",
    "    # direct full match\n",
    "    if cleaned in blocked:\n",
    "        return\n",
    "\n",
    "    # block if ANY token is a blocked word\n",
    "    if any(tok in blocked for tok in cleaned_tokens):\n",
    "        return\n",
    "\n",
    "    # block if phrase CONTAINS a blocked multi-word variant\n",
    "    for v in blocked:\n",
    "        if \" \" in v and v in cleaned:\n",
    "            return\n",
    "\n",
    "    # block if regex whole-word match\n",
    "    for v in blocked:\n",
    "        if re.search(rf\"\\b{re.escape(v)}\\b\", cleaned):\n",
    "            return\n",
    "\n",
    "    # only allow single words if forced (lexicon keep)\n",
    "    if force or len(cleaned_tokens) > 1:\n",
    "        descriptors.add(cleaned)\n",
    "\n",
    "\n",
    "# --- cleaning -- max 3 words, and 35 chars\n",
    "def clean_descriptor(phrase: str, max_words=3, max_chars=35, strict=True) -> str | None:\n",
    "    phrase = phrase.strip().lower()\n",
    "    words = phrase.split()\n",
    "\n",
    "    # too long\n",
    "    if len(words) > max_words:\n",
    "        return None\n",
    "    if len(phrase) > max_chars:\n",
    "        return None\n",
    "\n",
    "    # reject if contains non-letters (except spaces/underscores/hyphens)\n",
    "    if re.search(r\"[^a-z _-]\", phrase):\n",
    "        return None\n",
    "\n",
    "    # --- stopword logic ---\n",
    "    if strict:\n",
    "        # nuke phrase if ANY word is a stopword\n",
    "        if any(w in STOP_DESCRIPTORS for w in words):\n",
    "            return None\n",
    "    else:\n",
    "        # softer: only reject if whole phrase or first word is stopword\n",
    "        if phrase in STOP_DESCRIPTORS or words[0] in STOP_DESCRIPTORS:\n",
    "            return None\n",
    "\n",
    "    return phrase\n",
    "\n",
    "\n",
    "def extract_descriptors(text: str, english_name=None, latin_name=None, nlp_model=None):\n",
    "    if nlp_model is None:\n",
    "        nlp_model = nlp\n",
    "\n",
    "    blocked = expand_name_variants(english_name, latin_name)\n",
    "    doc = nlp_model(text)\n",
    "    descriptors = set()\n",
    "\n",
    "    for token in doc:\n",
    "        tok_lower = token.text.lower()\n",
    "\n",
    "        # adj+noun pairs\n",
    "        if token.dep_ == \"amod\" and token.head.pos_ == \"NOUN\":\n",
    "            maybe_add(f\"{token.text.lower()} {token.head.text.lower()}\", descriptors, blocked)\n",
    "\n",
    "        # compound nouns\n",
    "        if token.dep_ == \"compound\" and token.head.pos_ == \"NOUN\":\n",
    "            maybe_add(f\"{token.text.lower()} {token.head.text.lower()}\", descriptors, blocked)\n",
    "\n",
    "        # \"X-looking Y\" / \"X-like Y\"\n",
    "        if tok_lower in {\"looking\", \"like\"} and token.i > 0:\n",
    "            prev = doc[token.i - 1].text.lower()\n",
    "            head = (\n",
    "                doc[token.i + 1].text.lower()\n",
    "                if token.i + 1 < len(doc) and doc[token.i + 1].pos_ == \"NOUN\"\n",
    "                else token.head.text.lower()\n",
    "            )\n",
    "            maybe_add(f\"{prev}-{tok_lower} {head}\", descriptors, blocked)\n",
    "\n",
    "    # noun chunks (multi-word only)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        phrase = chunk.text.lower().strip()\n",
    "        if len(phrase.split()) > 1:\n",
    "            maybe_add(phrase, descriptors, blocked)\n",
    "\n",
    "    # lexicon force-keep (single words allowed)\n",
    "    for kw in LEXICON_TERMS:\n",
    "        if kw in text.lower():\n",
    "            maybe_add(kw, descriptors, blocked, force=True)\n",
    "\n",
    "    return sorted(descriptors)\n",
    "\n",
    "\n",
    "\n",
    "### tfidf scoring, boosting physical, and fish specific terms\n",
    "## keep species specifc terms\n",
    "def score_descriptors(descriptor_lists, top_n=30, nlp_model=None):\n",
    "    if nlp_model is None:\n",
    "        nlp_model = nlp\n",
    "\n",
    "    # normalize all descriptors\n",
    "    normalized_lists = []\n",
    "    mapping = {}\n",
    "    for lst in descriptor_lists:\n",
    "        normed = []\n",
    "        for d in lst:\n",
    "            norm = normalize_descriptor(d, nlp_model)\n",
    "            normed.append(norm)\n",
    "            if norm not in mapping:\n",
    "                mapping[norm] = d\n",
    "        normalized_lists.append(normed)\n",
    "\n",
    "    docs = [\" \".join([d.replace(\" \", \"_\") for d in lst]) for lst in normalized_lists]\n",
    "    vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", min_df=1)\n",
    "    tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    results = []\n",
    "    for doc_idx, doc in enumerate(docs):\n",
    "        scores = dict(zip(feature_names, tfidf_matrix[doc_idx].toarray().flatten()))\n",
    "\n",
    "        boosted = {}\n",
    "        for desc, score in scores.items():\n",
    "            desc_clean = desc.replace(\"_\", \" \")\n",
    "            representative = mapping.get(desc_clean, desc_clean)\n",
    "\n",
    "            # --- scoring logic ---\n",
    "            if representative in PHYSICAL_TERMS:\n",
    "                boost = 6.0   #  strong bias toward physical traits\n",
    "            elif representative in LEXICON_TERMS:\n",
    "                boost = 2.0\n",
    "            elif any(lex in representative for lex in LEXICON_TERMS):\n",
    "                boost = 3.0\n",
    "            else:\n",
    "                boost = 0.5   # downweight generic/behavioral stuff\n",
    "\n",
    "            boosted[representative] = score * boost\n",
    "\n",
    "        top = sorted(boosted.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        results.append([desc for desc, _ in top])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "## top 25 descriptors , new column\n",
    "\n",
    "def add_descriptor_columns(df: pd.DataFrame, top_n=25, nlp_model=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process transcript + wiki columns:\n",
    "    - Extract descriptors\n",
    "    - Score with TF-IDF + lexicon boosting\n",
    "    - Return top N per row\n",
    "    \"\"\"\n",
    "\n",
    "    if nlp_model is None:\n",
    "        nlp_model = nlp  # assumes you loaded spaCy globally\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) Extract raw descriptors (with blocking)\n",
    "    transcript_descs = []\n",
    "    wiki_descs = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        eng = row.get(\"english_name\", \"\")\n",
    "        lat = row.get(\"latin_name\", \"\")\n",
    "\n",
    "        t_desc = extract_descriptors(\n",
    "            str(row.get(\"transcript_embed\", \"\")),\n",
    "            english_name=eng,\n",
    "            latin_name=None,      # block English name\n",
    "            nlp_model=nlp_model,\n",
    "        )\n",
    "        w_desc = extract_descriptors(\n",
    "            str(row.get(\"wiki_desc_embed\", \"\")),\n",
    "            english_name=None,\n",
    "            latin_name=lat,       # block Latin name\n",
    "            nlp_model=nlp_model,\n",
    "        )\n",
    "\n",
    "        transcript_descs.append(t_desc)\n",
    "        wiki_descs.append(w_desc)\n",
    "\n",
    "    # 2) Score with TF-IDF + lexicon boost\n",
    "    transcript_top = score_descriptors(transcript_descs, top_n=top_n)\n",
    "    wiki_top = score_descriptors(wiki_descs, top_n=top_n)\n",
    "\n",
    "    # 3) Add back to DataFrame\n",
    "    df[\"transcript_top\"] = transcript_top\n",
    "    df[\"wiki_top\"] = wiki_top\n",
    "\n",
    "    return df\n",
    "\n",
    "#df1 = df.copy()\n",
    "#df1 = add_descriptor_columns(df1, top_n=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea73a7f1-b337-4189-bf00-3c3dee9abad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.to_csv('extracted_descriptors3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
